{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: RNNs in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals\n",
    "1. Build a simple RNN classifier\n",
    "2. Learn about PyTorch's in-built RNN modules (LSTM etc.)\n",
    "\n",
    "(Roughly follows http://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "from torchtext.vocab import Vectors, GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Building an RNN sentiment classifier\n",
    "#### Part 1.1: Generating the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll generate some toy data. The task will be to recall an integer at a certain position in a sequence. \n",
    "For a sequence a<sub>1</sub> a<sub>12</sub> a<sub>3</sub> a<sub>4</sub> a<sub>5</sub> the output might be a<sub>3</sub>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of training examples\n",
    "n_train = 2000\n",
    "\n",
    "# number of validation examples\n",
    "n_val = 1000\n",
    "\n",
    "# length of each sequence\n",
    "n_length = 10\n",
    "\n",
    "# examples per batch\n",
    "n_batch = 32\n",
    "\n",
    "# size of the vocabulary\n",
    "n_vocab = 20\n",
    "\n",
    "# position to be recalled\n",
    "answer_pos = n_length-1\n",
    "\n",
    "# generate random sequences\n",
    "train_seq = Variable(torch.Tensor(n_train, n_length).random_(0, n_vocab).long())\n",
    "val_seq = Variable(torch.Tensor(n_val, n_length).random_(0, n_vocab).long())\n",
    "\n",
    "# choose the correct labels\n",
    "train_labels = train_seq.clone()[:, answer_pos]\n",
    "val_labels = val_seq.clone()[:, answer_pos]\n",
    "\n",
    "# group data into batches\n",
    "train_iter = []\n",
    "for i in range(0, n_train, n_batch):\n",
    "    batch_seq = train_seq[i:i+n_batch]\n",
    "    batch_labels = train_labels[i:i+n_batch]\n",
    "    if (batch_seq.size()[0] == n_batch):\n",
    "        train_iter.append([batch_seq, batch_labels])\n",
    "    \n",
    "val_iter = []\n",
    "for i in range(0, n_val, n_batch):\n",
    "    batch_seq = val_seq[i:i+n_batch]\n",
    "    batch_labels = val_labels[i:i+n_batch]\n",
    "    if (batch_seq.size()[0] == n_batch):\n",
    "        val_iter.append([batch_seq, batch_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1.2 Build the model (version 1)\n",
    "\n",
    "The RNN module will be a PyTorch model like any other, with init a forward functions. This network:\n",
    "1. Takes as input the word at a particular point in the sequence, as well as the hidden state at the previous state of the network\n",
    "2. Uses nn.Embedding to get a vector for the word\n",
    "3. Concatenate the embedding and the hidden state\n",
    "4. Apply a linear layer to get the next hidden state\n",
    "5. Apply a linear layer to get the output\n",
    "6. Output both "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, vocab_size):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, input_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input)\n",
    "        combined = torch.cat((embedded, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1.3: Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can initialize and train the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_batch(model, criterion, optim, batch, label):\n",
    "    # initialize hidden vector\n",
    "    hidden = Variable(torch.zeros(n_batch, n_hidden))\n",
    "\n",
    "    # clear gradients\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    # calculate forward pass\n",
    "    for i in range(batch.size()[1]):\n",
    "        output, hidden = model(batch[:, i], hidden)\n",
    "\n",
    "    # calculate loss    \n",
    "    loss = criterion(output, label)\n",
    "\n",
    "    # backpropagate and step\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    \n",
    "    return loss.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "def train(model, criterion, optim):\n",
    "    for e in range(n_epochs):\n",
    "        batches = 0\n",
    "        epoch_loss = 0\n",
    "        avg_loss = 0\n",
    "        for batch, label in train_iter:\n",
    "            batch_loss = train_batch(model, criterion, optim, batch, label)\n",
    "            batches += 1\n",
    "            epoch_loss += batch_loss\n",
    "            avg_loss = ((avg_loss * (batches - 1)) + batch_loss) / batches\n",
    "        \n",
    "        print(\"Epoch \", e, \" Loss: \", epoch_loss)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0  Loss:  73.60259747505188\n",
      "Epoch  1  Loss:  43.383880376815796\n",
      "Epoch  2  Loss:  26.73484981060028\n",
      "Epoch  3  Loss:  17.657559633255005\n",
      "Epoch  4  Loss:  12.494984596967697\n",
      "Epoch  5  Loss:  9.371681913733482\n",
      "Epoch  6  Loss:  7.358956411480904\n",
      "Epoch  7  Loss:  5.98852775990963\n",
      "Epoch  8  Loss:  5.011282339692116\n",
      "Epoch  9  Loss:  4.287019722163677\n",
      "Epoch  10  Loss:  3.732741631567478\n",
      "Epoch  11  Loss:  3.2970485016703606\n",
      "Epoch  12  Loss:  2.9468020275235176\n",
      "Epoch  13  Loss:  2.659862741827965\n",
      "Epoch  14  Loss:  2.4209661558270454\n",
      "Epoch  15  Loss:  2.2192936949431896\n",
      "Epoch  16  Loss:  2.0469901897013187\n",
      "Epoch  17  Loss:  1.898227520287037\n",
      "Epoch  18  Loss:  1.7685999162495136\n",
      "Epoch  19  Loss:  1.6547195129096508\n",
      "Epoch  20  Loss:  1.5539430230855942\n",
      "Epoch  21  Loss:  1.4641783721745014\n",
      "Epoch  22  Loss:  1.3837508670985699\n",
      "Epoch  23  Loss:  1.3113047368824482\n",
      "Epoch  24  Loss:  1.2457311227917671\n",
      "Epoch  25  Loss:  1.186115127056837\n",
      "Epoch  26  Loss:  1.1316951923072338\n",
      "Epoch  27  Loss:  1.0818330589681864\n",
      "Epoch  28  Loss:  1.0359896179288626\n",
      "Epoch  29  Loss:  0.9937067367136478\n"
     ]
    }
   ],
   "source": [
    "# size of the hidden vector\n",
    "n_hidden = 3\n",
    "\n",
    "# initialize the network\n",
    "rnn = RNN(n_vocab, n_hidden, n_vocab, n_vocab)\n",
    "\n",
    "n_epochs = 30\n",
    "learning_rate = .05\n",
    "criterion = nn.NLLLoss()\n",
    "optim = torch.optim.SGD(rnn.parameters(), lr = learning_rate)\n",
    "\n",
    "train(rnn, criterion, optim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1.4: Test the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the model is similar to training it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_batch(batch, label):\n",
    "    if (batch.size()[0] != n_batch):\n",
    "        return 0, 0\n",
    "    \n",
    "    # initialize hidden state\n",
    "    hidden = Variable(torch.zeros(n_batch, n_hidden))\n",
    "    \n",
    "    # calculate forward pass\n",
    "    for i in range(batch[0].size()[0]):\n",
    "        output, hidden = rnn(batch[:, i], hidden)\n",
    "        \n",
    "    # calculate predictions\n",
    "    _, pred = output.max(1)\n",
    "\n",
    "    # calculate number of correct predictions\n",
    "    correct = (pred == label).long().sum().data[0]\n",
    "    return correct, n_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then calculate the total score by looping through the batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent correct:  0.9949596774193549\n"
     ]
    }
   ],
   "source": [
    "# Test loop\n",
    "\n",
    "batch_num = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "for i in range(len(val_iter)):\n",
    "    batch, label = val_iter[i]\n",
    "    batch_correct, batch_size = test_batch(batch, label)\n",
    "    batch_num += 1\n",
    "    correct += batch_correct\n",
    "    total += batch_size\n",
    "    \n",
    "print(\"Percent correct: \", correct / total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Using PyTorch RNN modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch's RNN capabilities live [here](http://pytorch.org/docs/master/nn.html#recurrent-layers). We can use it as follows (note that the input is batched along the **second** dimension):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "   0.2231 -0.0203 -0.0854 -0.2982 -0.0656  0.0930 -0.2293  0.6071 -0.0953\n",
      " -0.2261  0.1648 -0.5386  0.0828  0.2245 -0.2529 -0.0072 -0.2858  0.2296\n",
      " -0.0753 -0.3349 -0.4099  0.1039  0.0125  0.0802  0.2097 -0.1543 -0.2422\n",
      "\n",
      "Columns 9 to 17 \n",
      "  -0.1708  0.1044 -0.0797 -0.3872  0.1516  0.0719  0.0070 -0.2724 -0.0536\n",
      "  0.2972  0.1917 -0.3889  0.2385 -0.0287  0.0066 -0.0919  0.0148  0.1382\n",
      " -0.0847 -0.0421 -0.1286 -0.1308 -0.0199  0.1316  0.1084 -0.2401 -0.2508\n",
      "\n",
      "Columns 18 to 19 \n",
      "  -0.0918 -0.1930\n",
      "  0.0485 -0.0829\n",
      "  0.0594 -0.0006\n",
      "\n",
      "(1 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "   0.0831  0.0110 -0.0568 -0.2536  0.0285  0.0186 -0.1289  0.2785 -0.1063\n",
      " -0.1375  0.0868 -0.3380  0.0548  0.1371 -0.0536  0.0038 -0.0434  0.1007\n",
      " -0.1194 -0.1133 -0.4001  0.1453  0.1121  0.0408  0.0839  0.0398 -0.1405\n",
      "\n",
      "Columns 9 to 17 \n",
      "  -0.0189  0.0240 -0.1874 -0.3466  0.1337 -0.0032  0.0994 -0.1832 -0.1327\n",
      "  0.2092  0.0882 -0.3217 -0.0023 -0.0114  0.0006  0.0346 -0.0807  0.0927\n",
      "  0.0401  0.0249 -0.1558 -0.1999  0.0005  0.0335  0.1239 -0.1633 -0.1547\n",
      "\n",
      "Columns 18 to 19 \n",
      "  -0.0901 -0.1082\n",
      "  0.0146 -0.1045\n",
      "  0.0345  0.0067\n",
      "\n",
      "(2 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "   0.0391  0.0471 -0.0207 -0.1268  0.0612  0.0419 -0.0735  0.1910 -0.0928\n",
      " -0.0782  0.0689 -0.2186  0.0776  0.1349  0.0126 -0.0113  0.0469  0.0060\n",
      " -0.1022 -0.0306 -0.2528  0.1402  0.1364  0.0440  0.0259  0.0895 -0.0949\n",
      "\n",
      "Columns 9 to 17 \n",
      "   0.0431  0.0113 -0.1684 -0.2663  0.1363 -0.0509  0.1043 -0.1004 -0.1266\n",
      "  0.1793  0.0493 -0.2664 -0.1341  0.0230 -0.0273  0.0651 -0.0724  0.0381\n",
      "  0.1177  0.0227 -0.1636 -0.1887  0.0482 -0.0056  0.1302 -0.1123 -0.1042\n",
      "\n",
      "Columns 18 to 19 \n",
      "  -0.0836 -0.0356\n",
      " -0.0052 -0.0422\n",
      "  0.0073  0.0307\n",
      "\n",
      "(3 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "   0.0417  0.0551 -0.0139 -0.0409  0.0925  0.0635 -0.0079  0.1645 -0.0834\n",
      " -0.0271  0.0650 -0.1480  0.0641  0.1291  0.0282  0.0005  0.0798 -0.0317\n",
      " -0.0546  0.0147 -0.1543  0.1373  0.1342  0.0535  0.0089  0.1315 -0.0769\n",
      "\n",
      "Columns 9 to 17 \n",
      "   0.0954 -0.0049 -0.1616 -0.2353  0.1092 -0.0768  0.1295 -0.0786 -0.0887\n",
      "  0.1658  0.0087 -0.2348 -0.1632  0.0553 -0.0546  0.1169 -0.0616 -0.0161\n",
      "  0.1677  0.0124 -0.1518 -0.1725  0.0748 -0.0315  0.1274 -0.0979 -0.0437\n",
      "\n",
      "Columns 18 to 19 \n",
      "  -0.0614  0.0044\n",
      " -0.0192 -0.0201\n",
      " -0.0123  0.0549\n",
      "\n",
      "(4 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "   0.0181  0.0602 -0.0027  0.0157  0.0981  0.0742  0.0077  0.1315 -0.0807\n",
      "  0.0063  0.0715 -0.0915  0.0560  0.1247  0.0478  0.0022  0.0855 -0.0627\n",
      " -0.0113  0.0449 -0.0913  0.1318  0.1254  0.0678  0.0130  0.1444 -0.0661\n",
      "\n",
      "Columns 9 to 17 \n",
      "   0.1071 -0.0149 -0.1586 -0.1992  0.0996 -0.0962  0.1234 -0.0423 -0.0787\n",
      "  0.1520 -0.0160 -0.1985 -0.1479  0.0947 -0.0963  0.1331 -0.0386 -0.0441\n",
      "  0.1981 -0.0040 -0.1421 -0.1510  0.1066 -0.0570  0.1378 -0.0926 -0.0108\n",
      "\n",
      "Columns 18 to 19 \n",
      "  -0.0465  0.0306\n",
      " -0.0254  0.0175\n",
      " -0.0211  0.0701\n",
      "[torch.FloatTensor of size 5x3x20]\n",
      " (Variable containing:\n",
      "(0 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "   0.0221 -0.1683  0.0148  0.1591 -0.2388  0.1126 -0.1400 -0.0069 -0.1100\n",
      " -0.0477 -0.0093  0.0057  0.1716 -0.2316  0.0505 -0.1068  0.0392 -0.3080\n",
      "  0.0685 -0.1749 -0.1236  0.1825 -0.2149  0.0815 -0.1528 -0.2085 -0.1529\n",
      "\n",
      "Columns 9 to 17 \n",
      "   0.0114 -0.0883  0.0354  0.1227  0.1281  0.0109 -0.0549  0.0274 -0.0561\n",
      "  0.0839 -0.2424 -0.0200  0.2272 -0.0151  0.1424 -0.1468 -0.1098 -0.0787\n",
      " -0.2573 -0.3411 -0.1618 -0.0224 -0.1153 -0.1201 -0.3073 -0.1525 -0.0335\n",
      "\n",
      "Columns 18 to 19 \n",
      "   0.0838 -0.0764\n",
      " -0.0818  0.1260\n",
      "  0.0918  0.0291\n",
      "\n",
      "(1 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "   0.0181  0.0602 -0.0027  0.0157  0.0981  0.0742  0.0077  0.1315 -0.0807\n",
      "  0.0063  0.0715 -0.0915  0.0560  0.1247  0.0478  0.0022  0.0855 -0.0627\n",
      " -0.0113  0.0449 -0.0913  0.1318  0.1254  0.0678  0.0130  0.1444 -0.0661\n",
      "\n",
      "Columns 9 to 17 \n",
      "   0.1071 -0.0149 -0.1586 -0.1992  0.0996 -0.0962  0.1234 -0.0423 -0.0787\n",
      "  0.1520 -0.0160 -0.1985 -0.1479  0.0947 -0.0963  0.1331 -0.0386 -0.0441\n",
      "  0.1981 -0.0040 -0.1421 -0.1510  0.1066 -0.0570  0.1378 -0.0926 -0.0108\n",
      "\n",
      "Columns 18 to 19 \n",
      "  -0.0465  0.0306\n",
      " -0.0254  0.0175\n",
      " -0.0211  0.0701\n",
      "[torch.FloatTensor of size 2x3x20]\n",
      ", Variable containing:\n",
      "(0 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "   0.0401 -0.3045  0.0321  0.2665 -0.3502  0.2258 -0.3546 -0.0145 -0.1943\n",
      " -0.0841 -0.0146  0.0171  0.3851 -0.3564  0.1302 -0.2723  0.0562 -0.4485\n",
      "  0.1254 -0.2732 -0.2747  0.3770 -0.4480  0.3142 -0.2848 -0.4607 -0.2750\n",
      "\n",
      "Columns 9 to 17 \n",
      "   0.0192 -0.1368  0.0753  0.2989  0.3638  0.0198 -0.0960  0.0755 -0.1593\n",
      "  0.2049 -0.5452 -0.0633  0.4870 -0.0255  0.2064 -0.2808 -0.2666 -0.2347\n",
      " -0.5406 -0.8294 -0.5086 -0.0494 -0.2295 -0.2124 -0.6147 -0.3007 -0.0734\n",
      "\n",
      "Columns 18 to 19 \n",
      "   0.1458 -0.1278\n",
      " -0.1293  0.2803\n",
      "  0.1368  0.0639\n",
      "\n",
      "(1 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "   0.0344  0.1579 -0.0045  0.0272  0.1834  0.1914  0.0177  0.2503 -0.1870\n",
      "  0.0127  0.1998 -0.1509  0.0964  0.2372  0.1317  0.0051  0.1602 -0.1460\n",
      " -0.0212  0.1218 -0.1391  0.2259  0.2443  0.1628  0.0275  0.2772 -0.1565\n",
      "\n",
      "Columns 9 to 17 \n",
      "   0.2369 -0.0304 -0.3458 -0.3426  0.1787 -0.2034  0.2281 -0.0951 -0.1546\n",
      "  0.3473 -0.0325 -0.4311 -0.2479  0.1672 -0.1840  0.2472 -0.0844 -0.0826\n",
      "  0.4794 -0.0081 -0.3128 -0.2481  0.1923 -0.1166  0.2616 -0.2065 -0.0205\n",
      "\n",
      "Columns 18 to 19 \n",
      "  -0.0985  0.0616\n",
      " -0.0529  0.0352\n",
      " -0.0425  0.1327\n",
      "[torch.FloatTensor of size 2x3x20]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "n_input = 10\n",
    "n_hidden = 20\n",
    "n_layers = 2\n",
    "n_batch = 3\n",
    "n_length = 5\n",
    "rnn = nn.LSTM(n_input, n_hidden, n_layers)\n",
    "input = Variable(torch.randn(n_length, n_batch, n_input))\n",
    "h0 = Variable(torch.randn(n_layers, n_batch, n_hidden))\n",
    "c0 = Variable(torch.randn(n_layers, n_batch, n_hidden))\n",
    "output, hn = rnn(input, (h0, c0))\n",
    "print(output, hn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define a custom module to apply this module to our problem. This module will embed each integer, then apply the LSTM to the sequence, and then apply a linear and a softmax to get probabilities for each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyLSTM(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, output_size, vocab_size, n_layers):\n",
    "        super(MyLSTM, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, n_layers)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        # embed the input integers\n",
    "        embedded = self.embedding(input)\n",
    "        \n",
    "        # put the batch along the second dimension\n",
    "        embedded = embedded.transpose(0, 1)\n",
    "        \n",
    "        # apply the LSTM\n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        \n",
    "        # apply the linear and the softmax\n",
    "        output = self.softmax(self.linear(output))\n",
    "\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and testing are essentially the same as before, except that we no longer need to manually loop in the forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_batch(model, criterion, optim, batch, label):\n",
    "    # initialize hidden vectors\n",
    "    hidden = (Variable(torch.zeros(n_layers, n_batch, n_hidden)), Variable(torch.zeros(n_layers, n_batch, n_hidden)))\n",
    "\n",
    "    # clear gradients\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    # calculate forward pass\n",
    "    output, hidden = model(batch, hidden)\n",
    "\n",
    "    # calculate loss    \n",
    "    loss = criterion(output[answer_pos], label)\n",
    "\n",
    "    # backpropagate and step\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    \n",
    "    return loss.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training loop\n",
    "def train(model, criterion, optim):\n",
    "    for e in range(n_epochs):\n",
    "        batches = 0\n",
    "        epoch_loss = 0\n",
    "        avg_loss = 0\n",
    "        for batch, label in train_iter:\n",
    "            batch_loss = train_batch(model, criterion, optim, batch, label)\n",
    "            batches += 1\n",
    "            epoch_loss += batch_loss\n",
    "            avg_loss = ((avg_loss * (batches - 1)) + batch_loss) / batches\n",
    "        \n",
    "        print(\"Epoch \", e, \" Loss: \", epoch_loss)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0  Loss:  182.85484051704407\n",
      "Epoch  1  Loss:  117.944895029068\n",
      "Epoch  2  Loss:  78.74753439426422\n",
      "Epoch  3  Loss:  65.38788288831711\n",
      "Epoch  4  Loss:  60.50836080312729\n",
      "Epoch  5  Loss:  58.166617691516876\n",
      "Epoch  6  Loss:  56.78475075960159\n",
      "Epoch  7  Loss:  55.852413058280945\n",
      "Epoch  8  Loss:  55.16654431819916\n",
      "Epoch  9  Loss:  54.63217830657959\n",
      "Epoch  10  Loss:  54.199151039123535\n",
      "Epoch  11  Loss:  53.83822554349899\n",
      "Epoch  12  Loss:  53.530878841876984\n",
      "Epoch  13  Loss:  53.26531022787094\n",
      "Epoch  14  Loss:  53.03429317474365\n",
      "Epoch  15  Loss:  52.83198964595795\n",
      "Epoch  16  Loss:  52.65340077877045\n",
      "Epoch  17  Loss:  52.49481302499771\n",
      "Epoch  18  Loss:  52.35341036319733\n",
      "Epoch  19  Loss:  52.22695928812027\n",
      "Epoch  20  Loss:  52.11368536949158\n",
      "Epoch  21  Loss:  52.01213473081589\n",
      "Epoch  22  Loss:  51.92097508907318\n",
      "Epoch  23  Loss:  51.838898718357086\n",
      "Epoch  24  Loss:  51.76471447944641\n",
      "Epoch  25  Loss:  51.697418332099915\n",
      "Epoch  26  Loss:  51.636178970336914\n",
      "Epoch  27  Loss:  51.580303966999054\n",
      "Epoch  28  Loss:  51.52919465303421\n",
      "Epoch  29  Loss:  51.48234283924103\n"
     ]
    }
   ],
   "source": [
    "# size of the embeddings and vectors\n",
    "n_embedding = 128\n",
    "n_hidden = 128\n",
    "\n",
    "# number of layers\n",
    "n_layers = 1\n",
    "\n",
    "# initialize LSTM\n",
    "rnn = MyLSTM(n_embedding, n_hidden, n_vocab, n_vocab, n_layers)\n",
    "\n",
    "n_epochs = 30\n",
    "learning_rate = .1\n",
    "criterion = nn.NLLLoss()\n",
    "optim = torch.optim.SGD(rnn.parameters(), lr = learning_rate)\n",
    "\n",
    "train(rnn, criterion, optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent correct:  0.04939516129032258\n"
     ]
    }
   ],
   "source": [
    "# Test loop\n",
    "\n",
    "batch_num = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "for i in range(len(val_iter)):\n",
    "    batch, label = val_iter[i]\n",
    "    batch_correct, batch_size = test_batch(batch, label)\n",
    "    batch_num += 1\n",
    "    correct += batch_correct\n",
    "    total += batch_size\n",
    "    \n",
    "print(\"Percent correct: \", correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
